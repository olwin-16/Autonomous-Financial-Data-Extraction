{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c72edf9f-f835-49e9-82bd-67992ac42534",
      "metadata": {
        "id": "c72edf9f-f835-49e9-82bd-67992ac42534",
        "outputId": "54b67868-d908-47b3-8843-75872aecbbe2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install accelerate>=0.26.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea09aefd-d8d9-4210-97bd-0cc2a7ad7ae9",
      "metadata": {
        "id": "ea09aefd-d8d9-4210-97bd-0cc2a7ad7ae9",
        "outputId": "0ef5a1fd-018c-4685-c59a-7c05827e8bac",
        "colab": {
          "referenced_widgets": [
            "a0e9ddace12047f8b0a749cfa5dc4a4e",
            "05763f876f46478e9bd6951e84d06330",
            "323975d47c2e4e20ae130e5f9299df56",
            "8402e97e0cd9492c99a7851c3f8e0432"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a0e9ddace12047f8b0a749cfa5dc4a4e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: GET https://api.gradio.app/gradio-messaging/en \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "05763f876f46478e9bd6951e84d06330",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/56.5k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "323975d47c2e4e20ae130e5f9299df56",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8402e97e0cd9492c99a7851c3f8e0432",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00004.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
        "import gradio as gr\n",
        "from playwright.sync_api import sync_playwright\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import logging\n",
        "import os  # Added for environment variable\n",
        "from accelerate import init_empty_weights\n",
        "\n",
        "# Set a writable Hugging Face cache directory\n",
        "os.environ[\"TRANSFORMERS_CACHE\"] = \"./hf_cache\"\n",
        "os.environ[\"HF_HOME\"] = \"./hf_cache\"\n",
        "\n",
        "# Disable Hugging Face telemetry warnings\n",
        "os.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = \"1\"\n",
        "\n",
        "# Logger setup\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# 1️⃣ Load UI-TARS-7B-DPO Model (CPU-Compatible for Colab)\n",
        "model_name = \"bytedance-research/UI-TARS-7B-DPO\"\n",
        "device = \"cpu\"  # Force CPU usage in Colab\n",
        "\n",
        "# Ensure Accelerate is used for efficient CPU loading\n",
        "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float32,  # Use FP32 for CPU compatibility\n",
        "    low_cpu_mem_usage=True,  # Optimize memory usage\n",
        "    device_map=\"auto\"  # Automatically assign device\n",
        ")\n",
        "\n",
        "# Load processor\n",
        "processor = AutoProcessor.from_pretrained(model_name)\n",
        "logger.info(\"UI-TARS-7B-DPO Model Loaded Successfully on CPU!\")\n",
        "\n",
        "def generate_response(prompt):\n",
        "    inputs = processor(prompt, return_tensors=\"pt\").to(device)\n",
        "    output = model.generate(**inputs, max_length=512)\n",
        "    return processor.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# 2️⃣ Web Automation Setup\n",
        "def browse_website(url):\n",
        "    with sync_playwright() as p:\n",
        "        browser = p.chromium.launch(headless=True)\n",
        "        page = browser.new_page()\n",
        "        page.goto(url)\n",
        "        screenshot_path = \"screenshot.png\"\n",
        "        page.screenshot(path=screenshot_path)\n",
        "        browser.close()\n",
        "    return screenshot_path\n",
        "\n",
        "# 3️⃣ Extract Webpage Text\n",
        "def extract_text_from_url(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    text = ' '.join([p.text for p in soup.find_all('p')])\n",
        "    return text\n",
        "\n",
        "def answer_question(url, question):\n",
        "    page_text = extract_text_from_url(url)\n",
        "    prompt = f\"Based on the following webpage content:\\n\\n{page_text}\\n\\nAnswer this question: {question}\"\n",
        "    response = generate_response(prompt)\n",
        "    return response\n",
        "\n",
        "# 4️⃣ UI with Gradio\n",
        "def gradio_interface(url, question):\n",
        "    screenshot_path = browse_website(url)\n",
        "    response = answer_question(url, question)\n",
        "    return screenshot_path, response\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=gradio_interface,\n",
        "    inputs=[gr.Textbox(label=\"Enter Website URL\"), gr.Textbox(label=\"Ask a Question\")],\n",
        "    outputs=[gr.Image(label=\"Captured UI\"), gr.Textbox(label=\"Model Answer\")],\n",
        "    title=\"UI TARS Web Agent\",\n",
        "    description=\"Enter a website URL and a question to analyze the webpage and get answers.\"\n",
        ")\n",
        "\n",
        "iface.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b994cd2-dca0-4adc-9d20-886320853d2a",
      "metadata": {
        "id": "8b994cd2-dca0-4adc-9d20-886320853d2a"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}